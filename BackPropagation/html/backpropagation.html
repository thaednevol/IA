
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>RED NEURONAL BACKPROPAGATION PARA UNA COMPUERTA XOR</title><meta name="generator" content="MATLAB 9.1"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-06-06"><meta name="DC.source" content="backpropagation.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>RED NEURONAL BACKPROPAGATION PARA UNA COMPUERTA XOR</h1><!--introduction--><p>El algoritmo Backpropagation es un m&eacute;todo de aprendizaje supervisado para redes Feed-Forward multicapa para el campo de Redes Neuronales Artificiales.</p><p>Las redes Feed-forward se inspiran en el procesamiento de informaci&oacute;n de una o m&aacute;s c&eacute;lulas neuronales, llamadas neuronas. Una neurona acepta se&ntilde;ales de entrada a trav&eacute;s de sus dendritas, que pasan la se&ntilde;al el&eacute;ctrica hacia el cuerpo celular. El ax&oacute;n lleva lleva la se&ntilde;al a las sinapsis, que son las conexiones del ax&oacute;n de una c&eacute;lula con las dendritas de otras c&eacute;lulas.</p><p>El principio del enfoque de backpropagation es modelar una funci&oacute;n dada modificando las ponderaciones internas de las se&ntilde;ales de entrada para producir una se&ntilde;al de salida esperada. El sistema es entrenado usando un m&eacute;todo de aprendizaje supervisado, donde el error entre la salida del sistema y una salida esperada conocida se presenta al sistema y se utiliza para modificar su estado interno.</p><p>T&eacute;cnicamente, el algoritmo backpropagation es un m&eacute;todo para entrenar los pesos en una red neuronal de avance m&uacute;ltiple. Como tal, requiere que se defina una estructura de red de una o m&aacute;s capas en las que una capa est&aacute; totalmente conectada a la capa siguiente. Una estructura de red est&aacute;ndar es una capa de entrada, una capa oculta y una capa de salida.</p><p>Backpropagation puede usarse para problemas de clasificaci&oacute;n y regresi&oacute;n. En los problemas de clasificaci&oacute;n, los mejores resultados se consiguen cuando la red tiene una neurona en la capa de salida para cada valor de clase. Por ejemplo, un problema de clasificaci&oacute;n de 2 clases o binario con los valores de clase de A y B. Estos resultados esperados tendr&iacute;an que ser transformados en vectores binarios con una columna para cada valor de clase. Tales como [1, 0] y [0, 1] para A y B, respectivamente. A esto se llama una codificaci&oacute;n caliente.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#22">MODIFICACION DE ALPHA</a></li><li><a href="#24">MODIFICACION BETA</a></li><li><a href="#26">CONCLUSIONES</a></li></ul></div><pre class="codeinput">clear <span class="string">all</span>;
close <span class="string">all</span>;
clc;
</pre><p>Primero se asignan los pesos</p><pre class="codeinput"><span class="comment">%Wi=[rand rand rand;rand rand rand;rand rand rand];</span>
Wi=[0.133503859661312 0.021555887203497 0.559840705872510;0.300819018069489 0.939409713873458 0.980903636046859;0.286620388894259 0.800820286951535 0.896111351432604];
Wx=Wi;
</pre><pre>A continuaci&oacute;n se definen las entradas</pre><pre class="codeinput">x1=[0 0 1 1];
x2=[0 1 0 1];

Xi = [x1;x2];
</pre><pre>Se definen las salidas</pre><pre class="codeinput">Yi=[0 1 1 0];

A=[Xi(1,1),Xi(2,1),Yi(1);
   Xi(1,2),Xi(2,2),Yi(2);
   Xi(1,3),Xi(2,3),Yi(3);
   Xi(1,4),Xi(2,4),Yi(4)];
</pre><p>Se usar&aacute; un par&aacute;metro llamado tasa de aprendizaje</p><pre class="codeinput">alpha=0.7;
</pre><p>Adem&aacute;s se usa un valor de sesgo, permite cambiar la funci&oacute;n de activaci&oacute;n a la izquierda o a la derecha, lo que puede ser cr&iacute;tico para el aprendizaje exitoso.</p><pre class="codeinput">beta=1;
</pre><p>Se va a usar la diferencia (t) entre el valor superior e inferior de la salida, para determinar cu&aacute;ndo es un uno o un cero</p><pre class="codeinput">t=0.8;
</pre><p>Se definen unas variables de control y de conteo</p><pre class="codeinput">d=0; <span class="comment">%Variable de control para verificar la diferencia entre la salida m&aacute;xima y la m&iacute;nima</span>
iteraciones=1;
Yf=zeros(1,4);
error=zeros();
</pre><p>El algoritmo usado es implementado en una funci&oacute;n aparte (bp_metodo1).  La f&oacute;rmula usada para calcular la salida es la funci&oacute;n sigmoidal, la cual es totalmente derivable, e igualmente ser&aacute; implementada en una funci&oacute;n aparte (sigma)</p><pre class="codeinput">disp(<span class="string">'Valores normales'</span>);
<span class="keyword">for</span> j=1:length(Yi)
    H1 = beta*Wi(1,1) + Xi(1,j)*Wi(1,2) + Xi(2,j)*Wi(1,3);
    O1=sigma(H1);
    H2 = beta*Wi(2,1) + Xi(1,j)*Wi(2,2) + Xi(2,j)*Wi(2,3);

    O2=1/(1+exp(-H2));
    H3 = beta*Wi(3,1) + O1*Wi(3,2) + O2*Wi(3,3);
    Yf(j)=sigma(H3);
    error(j)=Yi(j) - Yf(j);
    d3_1= Yf(j) * (1 - Yf(j)) * error(j);
    d2_1=O1 * (1-O1) * Wi(3,2) * d3_1;
    d2_2=O2 * (1-O2) * Wi(3,3) * d3_1;
    <span class="keyword">for</span> k = 1:3
<span class="comment">%       Discriminaci&oacute;n de los sesgos</span>
        <span class="keyword">if</span> k == 1
            Wi(1,k) = Wi(1,k) + alpha * beta * d2_1;
            Wi(2,k) = Wi(2,k) + alpha * beta * d2_2;
            Wi(3,k) = Wi(3,k) + alpha * beta * d3_1;
<span class="comment">%        Discriminaci&oacute;n de las neuronas (k&gt;1)</span>
        <span class="keyword">else</span>
            Wi(1,k) = Wi(1,k) + alpha*Xi(1,j)*d2_1;
            Wi(2,k) = Wi(2,k) + alpha*Xi(2,j)*d2_2;
            <span class="keyword">if</span> (k==2)
                Wi(3,k) = Wi(3,k) + alpha*O1*d3_1;
            <span class="keyword">else</span>
                Wi(3,k) = Wi(3,k) + alpha*O2*d3_1;
            <span class="keyword">end</span>
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">Valores normales
</pre><p>Se eval&uacute;a la suma del error cuadr&aacute;tico medio</p><pre class="codeinput">ec=sum(error.^2)/2;

<span class="keyword">if</span> d==abs(max(Yf)-min(Yf))
    <span class="keyword">return</span>;
<span class="keyword">else</span>
    d=abs(max(Yf)-min(Yf));
<span class="keyword">end</span>
res(1,:)=[Yf error ec d];
</pre><p>Se repite el algoritmo hasta que la diferencia sea menor que la establecida previamente Debido a que, para ciertos valores la respuesta o converge, se han creado los siguientes valores para validar la ejecuci&oacute;n:</p><pre class="codeinput">b=0;
jj=0;
kk=0;
rep=100;

<span class="keyword">while</span> d&lt;t
    iteraciones=iteraciones+1;
    [Yf,Wi,ex,ec,d,jj,kk,b]=calculateBP(Xi,Wi,Yi,alpha,beta,Yf,d,b,jj,kk);
    <span class="keyword">if</span> kk&gt;rep
        <span class="keyword">break</span>
    <span class="keyword">end</span>
    res(iteraciones,:)=[Yf ex ec d];
<span class="keyword">end</span>

figure(<span class="string">'units'</span>,<span class="string">'normalized'</span>,<span class="string">'outerposition'</span>,[0 0 1 1])
x=1:1:size(res,1);
subplot(121)
hold <span class="string">on</span>
plot(x,res(:,1),<span class="string">':r.'</span>)
plot(x,res(:,2),<span class="string">':g.'</span>)
plot(x,res(:,3),<span class="string">':b.'</span>)
plot(x,res(:,4),<span class="string">':y.'</span>)
title(<span class="string">'Salida vs Iteraciones'</span>)
xlabel(<span class="string">'Iteraciones'</span>)
ylabel(<span class="string">'Valor Yf'</span>)
legend(<span class="string">'Yf(1)'</span>,<span class="string">'Yf(2)'</span>, <span class="string">'Yf(3)'</span>,<span class="string">'Yf(4)'</span>,<span class="string">'Location'</span>,<span class="string">'northwest'</span>)
grid <span class="string">on</span>
subplot(122)
hold <span class="string">on</span>
plot(x,res(:,5),<span class="string">':r.'</span>)
plot(x,res(:,6),<span class="string">':g.'</span>)
plot(x,res(:,7),<span class="string">':b.'</span>)
plot(x,res(:,8),<span class="string">':y.'</span>)
title(<span class="string">'Error vs Iteraciones'</span>)
xlabel(<span class="string">'Iteraciones'</span>)
ylabel(<span class="string">'Error'</span>)
legend(<span class="string">'Yf(1)'</span>,<span class="string">'Yf(2)'</span>, <span class="string">'Yf(3)'</span>,<span class="string">'Yf(4)'</span>,<span class="string">'Location'</span>,<span class="string">'northwest'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="backpropagation_01.png" alt=""> <p>Un ejemplo de valores no convergentes se da cuando los pesos se inicializan as&iacute;:</p><pre class="codeinput"><span class="comment">%beta=-1</span>
<span class="comment">%Wi=[0.052676997680793 0.737858095516997 0.269119426398556;0.422835615008808 0.547870901214845 0.942736984276934;0.417744104316662 0.983052466469856 0.301454948712065];</span>
<span class="comment">%beta=0</span>
<span class="comment">%Wi=[0.308914593566815 0.726104431664832 0.782872072979123;0.693787614986897 0.009802252263062 0.843213338010510;0.922331997796276 0.770954220673925 0.042659855935049]</span>
disp(<span class="string">'Valores no convergentes'</span>);
Wi=[0.895891573792568 0.099089649681815 0.044165571762661;0.557295155804762 0.772495067187624 0.311940057496269;0.178982479314335 0.338955678247718 0.210145637043552];
rep=100;
iteraciones=0;
Yf=zeros(1,4);
error=zeros();
b=0;
jj=0;
kk=0;
d=0;
<span class="keyword">while</span> d&lt;t
    iteraciones=iteraciones+1;
    [Yf,Wi,ex,ec,d,jj,kk,b]=calculateBP(Xi,Wi,Yi,alpha,beta,Yf,d,b,jj,kk);
    <span class="keyword">if</span> kk&gt;rep
        <span class="keyword">break</span>
    <span class="keyword">end</span>
    res(iteraciones,:)=[Yf ex ec d];
<span class="keyword">end</span>

figure(<span class="string">'units'</span>,<span class="string">'normalized'</span>,<span class="string">'outerposition'</span>,[0 0 1 1])
x=1:1:size(res,1);
subplot(121)
hold <span class="string">on</span>
plot(x,res(:,1),<span class="string">':r.'</span>)
plot(x,res(:,2),<span class="string">':g.'</span>)
plot(x,res(:,3),<span class="string">':b.'</span>)
plot(x,res(:,4),<span class="string">':y.'</span>)
title(<span class="string">'Salida vs Iteraciones'</span>)
xlabel(<span class="string">'Iteraciones'</span>)
ylabel(<span class="string">'Valor Yf'</span>)
legend(<span class="string">'Yf(1)'</span>,<span class="string">'Yf(2)'</span>, <span class="string">'Yf(3)'</span>,<span class="string">'Yf(4)'</span>,<span class="string">'Location'</span>,<span class="string">'northwest'</span>)
grid <span class="string">on</span>
subplot(122)
hold <span class="string">on</span>
plot(x,res(:,5),<span class="string">':r.'</span>)
plot(x,res(:,6),<span class="string">':g.'</span>)
plot(x,res(:,7),<span class="string">':b.'</span>)
plot(x,res(:,8),<span class="string">':y.'</span>)
title(<span class="string">'Error vs Iteraciones'</span>)
xlabel(<span class="string">'Iteraciones'</span>)
ylabel(<span class="string">'Error'</span>)
legend(<span class="string">'Yf(1)'</span>,<span class="string">'Yf(2)'</span>, <span class="string">'Yf(3)'</span>,<span class="string">'Yf(4)'</span>,<span class="string">'Location'</span>,<span class="string">'northwest'</span>)
grid <span class="string">on</span>
</pre><pre class="codeoutput">Valores no convergentes
</pre><img vspace="5" hspace="5" src="backpropagation_02.png" alt=""> <h2 id="22">MODIFICACION DE ALPHA</h2><p>Se modifica la tasa de aprendizaje. Se toman los pesos inicializados anteriormente</p><pre class="codeinput">disp(<span class="string">'Modificacion de alpha'</span>);
indice=1;
beta=1;

<span class="keyword">for</span> alpha=1:0.01:2
    Wi=Wx;
    iteraciones=0;
    Yf=zeros(1,4);
    error=zeros();
    b=0;
    jj=0;
    kk=0;
    d=0;
    res=[];
    <span class="keyword">while</span> d&lt;t
        tic;
        iteraciones=iteraciones+1;
        [Yf,Wf,ex,ec,d,jj,kk,b]=calculateBP(Xi,Wi,Yi,alpha,beta,Yf,d,b,jj,kk);
        Wi=Wf;
        <span class="keyword">if</span> kk&gt;rep
            <span class="keyword">break</span>
        <span class="keyword">end</span>
        res(iteraciones,:)=[Yf ex ec d];
        tiempo_usado=toc;
    <span class="keyword">end</span>
    Va(indice,1)=[ec];
    Va(indice,2)=[tiempo_usado];
    Va(indice,3)=[iteraciones];
    Va(indice,4)=abs(Wx(9)-Wf(9));
    indice=indice+1;
<span class="keyword">end</span>

x=1:0.01:2;
figure(<span class="string">'units'</span>,<span class="string">'normalized'</span>,<span class="string">'outerposition'</span>,[0 0 1 1])
subplot(2,2,1)
y1=Va(:,1);
plot(x,y1);
grid <span class="string">on</span>
title(<span class="string">'Error cuadr&aacute;tico vs alpha'</span>)

subplot(2,2,2)
y2=Va(:,2);
plot(x,y2);
grid <span class="string">on</span>
title(<span class="string">'Tiempo usado vs alpha'</span>)

subplot(2,2,3)
y3=Va(:,3);
plot(x,y3);
grid <span class="string">on</span>
title(<span class="string">'Iteraciones vs alpha'</span>)

subplot(2,2,4)
y4=Va(:,4);
plot(x,y4);
grid <span class="string">on</span>
title(<span class="string">'Delta peso neurona salida vs alpha'</span>)
</pre><pre class="codeoutput">Modificacion de alpha
</pre><img vspace="5" hspace="5" src="backpropagation_03.png" alt=""> <h2 id="24">MODIFICACION BETA</h2><pre class="codeinput">disp(<span class="string">'Modificacion de beta'</span>);
indice=1;
alpha=0.5;
<span class="keyword">for</span> beta=0.1:0.1:1
    Wi=Wx;
    iteraciones=0;
    Yf=zeros(1,4);
    error=zeros();
    b=0;
    jj=0;
    kk=0;
    d=0;
    Wp=Wi;
    res=[];
    <span class="keyword">while</span> d&lt;t
        tic;
        iteraciones=iteraciones+1;
        [Yf,Wf,ex,ec,d,jj,kk,b]=calculateBP(Xi,Wi,Yi,alpha,beta,Yf,d,b,jj,kk);
        Wi=Wf;
        <span class="keyword">if</span> kk&gt;rep
            <span class="keyword">break</span>
        <span class="keyword">end</span>
        res(iteraciones,:)=[Yf ex ec d];
        tiempo_usado=toc;
    <span class="keyword">end</span>
    Ya(indice,1)=[ec];
    Ya(indice,2)=[tiempo_usado];
    Ya(indice,3)=[iteraciones];
    Ya(indice,4)=abs(Wx(9)-Wf(9));
    indice=indice+1;
<span class="keyword">end</span>

x=0.1:0.1:1;
figure(<span class="string">'units'</span>,<span class="string">'normalized'</span>,<span class="string">'outerposition'</span>,[0 0 1 1])
subplot(2,2,1)
y1=Ya(:,1);
plot(x,y1);
grid <span class="string">on</span>
title(<span class="string">'Error cuadr&aacute;tico vs beta'</span>)

subplot(2,2,2)
y2=Ya(:,2);
plot(x,y2);
grid <span class="string">on</span>
title(<span class="string">'Tiempo usado vs beta'</span>)

subplot(2,2,3)
y3=Ya(:,3);
plot(x,y3);
grid <span class="string">on</span>
title(<span class="string">'Iteraciones vs beta'</span>)

subplot(2,2,4)
y4=Ya(:,4);
plot(x,y4);
grid <span class="string">on</span>
title(<span class="string">'Delta peso neurona salida vs beta'</span>)
</pre><pre class="codeoutput">Modificacion de beta
</pre><img vspace="5" hspace="5" src="backpropagation_04.png" alt=""> <h2 id="26">CONCLUSIONES</h2><div><ol><li>En t&eacute;rminos generales, se debe establecer un criterio para asignar la diferencia entre las salidas, el n&uacute;mero de pasos, la tasa de aprendizaje y la polarizaci&oacute;n, acorde a las necesidades para tener el resultado deseado</li><li>Para el valor seleccionado de alpha (0,5) los mejores valores de beta se encuentran entre 0.3 y 0.4</li><li>Para el valor seleccionado de beta (1) los mejores valores de alpha se encuentran entre 0.3 y 0.4</li></ol></div><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% RED NEURONAL BACKPROPAGATION PARA UNA COMPUERTA XOR
% 
% El algoritmo Backpropagation es un método de aprendizaje supervisado para redes Feed-Forward multicapa para el campo de Redes
% Neuronales Artificiales.
%
% Las redes Feed-forward se inspiran en el procesamiento de información de una o más células neuronales, llamadas neuronas. Una
% neurona acepta señales de entrada a través de sus dendritas, que pasan la señal eléctrica hacia el cuerpo celular. El axón lleva
% lleva la señal a las sinapsis, que son las conexiones del axón de una célula con las dendritas de otras células.
%
% El principio del enfoque de backpropagation es modelar una función dada modificando las ponderaciones internas de las señales de
% entrada para producir una señal de salida esperada. El sistema es entrenado usando un método de aprendizaje supervisado, donde
% el error entre la salida del sistema y una salida esperada conocida se presenta al sistema y se utiliza para modificar su estado
% interno.
%
% Técnicamente, el algoritmo backpropagation es un método para entrenar los pesos en una red neuronal de avance múltiple. Como
% tal, requiere que se defina una estructura de red de una o más capas en las que una capa está totalmente conectada a la capa
% siguiente. Una estructura de red estándar es una capa de entrada, una capa oculta y una capa de salida.
% 
% Backpropagation puede usarse para problemas de clasificación y regresión. En los problemas de clasificación, los mejores
% resultados se consiguen cuando la red tiene una neurona en la capa de salida para cada valor de clase. Por ejemplo, un problema
% de clasificación de 2 clases o binario con los valores de clase de A y B. Estos resultados esperados tendrían que ser
% transformados en vectores binarios con una columna para cada valor de clase. Tales como [1, 0] y [0, 1] para A y B,
% respectivamente. A esto se llama una codificación caliente. 
%%
clear all;
close all;
clc;
%%
% Primero se asignan los pesos
%%
%Wi=[rand rand rand;rand rand rand;rand rand rand];
Wi=[0.133503859661312 0.021555887203497 0.559840705872510;0.300819018069489 0.939409713873458 0.980903636046859;0.286620388894259 0.800820286951535 0.896111351432604];
Wx=Wi;
%%
%  A continuación se definen las entradas
%%
x1=[0 0 1 1];
x2=[0 1 0 1];

Xi = [x1;x2];

%%
%  Se definen las salidas
%%
Yi=[0 1 1 0];

A=[Xi(1,1),Xi(2,1),Yi(1);
   Xi(1,2),Xi(2,2),Yi(2);
   Xi(1,3),Xi(2,3),Yi(3);
   Xi(1,4),Xi(2,4),Yi(4)];

%%
% Se usará un parámetro llamado tasa de aprendizaje
%%
alpha=0.7;
%%
% Además se usa un valor de sesgo, permite cambiar la función de activación a la izquierda o a la derecha, lo que puede ser
% crítico para el aprendizaje exitoso.
%%
beta=1;
%%
% Se va a usar la diferencia (t) entre el valor superior e inferior de la salida, para determinar cuándo es un uno o un cero
%%
t=0.8;
%%
% Se definen unas variables de control y de conteo
%%
d=0; %Variable de control para verificar la diferencia entre la salida máxima y la mínima
iteraciones=1;
Yf=zeros(1,4);
error=zeros();
%%
% El algoritmo usado es implementado en una función aparte (bp_metodo1).  La fórmula usada para calcular la salida es la función
% sigmoidal, la cual es totalmente derivable, e igualmente será implementada en una función aparte (sigma)
%%
disp('Valores normales');
for j=1:length(Yi)
    H1 = beta*Wi(1,1) + Xi(1,j)*Wi(1,2) + Xi(2,j)*Wi(1,3);
    O1=sigma(H1);
    H2 = beta*Wi(2,1) + Xi(1,j)*Wi(2,2) + Xi(2,j)*Wi(2,3);
    
    O2=1/(1+exp(-H2));
    H3 = beta*Wi(3,1) + O1*Wi(3,2) + O2*Wi(3,3);
    Yf(j)=sigma(H3);
    error(j)=Yi(j) - Yf(j);
    d3_1= Yf(j) * (1 - Yf(j)) * error(j);
    d2_1=O1 * (1-O1) * Wi(3,2) * d3_1;
    d2_2=O2 * (1-O2) * Wi(3,3) * d3_1;
    for k = 1:3
%       Discriminación de los sesgos
        if k == 1
            Wi(1,k) = Wi(1,k) + alpha * beta * d2_1;
            Wi(2,k) = Wi(2,k) + alpha * beta * d2_2;
            Wi(3,k) = Wi(3,k) + alpha * beta * d3_1;
%        Discriminación de las neuronas (k>1)
        else
            Wi(1,k) = Wi(1,k) + alpha*Xi(1,j)*d2_1;
            Wi(2,k) = Wi(2,k) + alpha*Xi(2,j)*d2_2;
            if (k==2)
                Wi(3,k) = Wi(3,k) + alpha*O1*d3_1;
            else
                Wi(3,k) = Wi(3,k) + alpha*O2*d3_1;
            end
        end
    end
end
%%
% Se evalúa la suma del error cuadrático medio
%
ec=sum(error.^2)/2;

if d==abs(max(Yf)-min(Yf))
    return;
else
    d=abs(max(Yf)-min(Yf));
end
res(1,:)=[Yf error ec d];

%%
% Se repite el algoritmo hasta que la diferencia sea menor que la establecida previamente
% Debido a que, para ciertos valores la respuesta o converge, se han creado los siguientes valores para validar la ejecución:

b=0;
jj=0;
kk=0;
rep=100;

while d<t
    iteraciones=iteraciones+1;
    [Yf,Wi,ex,ec,d,jj,kk,b]=calculateBP(Xi,Wi,Yi,alpha,beta,Yf,d,b,jj,kk);
    if kk>rep
        break
    end
    res(iteraciones,:)=[Yf ex ec d];
end

figure('units','normalized','outerposition',[0 0 1 1])
x=1:1:size(res,1);
subplot(121)
hold on
plot(x,res(:,1),':r.')
plot(x,res(:,2),':g.')
plot(x,res(:,3),':b.')
plot(x,res(:,4),':y.')
title('Salida vs Iteraciones')
xlabel('Iteraciones') 
ylabel('Valor Yf') 
legend('Yf(1)','Yf(2)', 'Yf(3)','Yf(4)','Location','northwest')
grid on
subplot(122)
hold on
plot(x,res(:,5),':r.')
plot(x,res(:,6),':g.')
plot(x,res(:,7),':b.')
plot(x,res(:,8),':y.')
title('Error vs Iteraciones')
xlabel('Iteraciones') 
ylabel('Error') 
legend('Yf(1)','Yf(2)', 'Yf(3)','Yf(4)','Location','northwest')
grid on
%%
% Un ejemplo de valores no convergentes se da cuando los pesos se inicializan así:
%%
%beta=-1
%Wi=[0.052676997680793 0.737858095516997 0.269119426398556;0.422835615008808 0.547870901214845 0.942736984276934;0.417744104316662 0.983052466469856 0.301454948712065];
%beta=0
%Wi=[0.308914593566815 0.726104431664832 0.782872072979123;0.693787614986897 0.009802252263062 0.843213338010510;0.922331997796276 0.770954220673925 0.042659855935049]
disp('Valores no convergentes');
Wi=[0.895891573792568 0.099089649681815 0.044165571762661;0.557295155804762 0.772495067187624 0.311940057496269;0.178982479314335 0.338955678247718 0.210145637043552];
rep=100;
iteraciones=0;
Yf=zeros(1,4);
error=zeros();
b=0;
jj=0;
kk=0;
d=0;
while d<t
    iteraciones=iteraciones+1;
    [Yf,Wi,ex,ec,d,jj,kk,b]=calculateBP(Xi,Wi,Yi,alpha,beta,Yf,d,b,jj,kk);
    if kk>rep
        break
    end
    res(iteraciones,:)=[Yf ex ec d];
end

figure('units','normalized','outerposition',[0 0 1 1])
x=1:1:size(res,1);
subplot(121)
hold on
plot(x,res(:,1),':r.')
plot(x,res(:,2),':g.')
plot(x,res(:,3),':b.')
plot(x,res(:,4),':y.')
title('Salida vs Iteraciones')
xlabel('Iteraciones') 
ylabel('Valor Yf') 
legend('Yf(1)','Yf(2)', 'Yf(3)','Yf(4)','Location','northwest')
grid on
subplot(122)
hold on
plot(x,res(:,5),':r.')
plot(x,res(:,6),':g.')
plot(x,res(:,7),':b.')
plot(x,res(:,8),':y.')
title('Error vs Iteraciones')
xlabel('Iteraciones') 
ylabel('Error') 
legend('Yf(1)','Yf(2)', 'Yf(3)','Yf(4)','Location','northwest')
grid on
%% MODIFICACION DE ALPHA
% Se modifica la tasa de aprendizaje. Se toman los pesos inicializados anteriormente
%%
disp('Modificacion de alpha');
indice=1;
beta=1;

for alpha=1:0.01:2
    Wi=Wx;
    iteraciones=0;
    Yf=zeros(1,4);
    error=zeros();
    b=0;
    jj=0;
    kk=0;
    d=0;
    res=[];
    while d<t
        tic;
        iteraciones=iteraciones+1;
        [Yf,Wf,ex,ec,d,jj,kk,b]=calculateBP(Xi,Wi,Yi,alpha,beta,Yf,d,b,jj,kk);
        Wi=Wf;
        if kk>rep
            break
        end
        res(iteraciones,:)=[Yf ex ec d];
        tiempo_usado=toc;
    end
    Va(indice,1)=[ec];
    Va(indice,2)=[tiempo_usado];
    Va(indice,3)=[iteraciones];
    Va(indice,4)=abs(Wx(9)-Wf(9));
    indice=indice+1;
end

x=1:0.01:2;
figure('units','normalized','outerposition',[0 0 1 1])
subplot(2,2,1)
y1=Va(:,1);
plot(x,y1);
grid on
title('Error cuadrático vs alpha')

subplot(2,2,2)
y2=Va(:,2);
plot(x,y2);
grid on
title('Tiempo usado vs alpha')

subplot(2,2,3)
y3=Va(:,3);
plot(x,y3);
grid on
title('Iteraciones vs alpha')

subplot(2,2,4)
y4=Va(:,4);
plot(x,y4);
grid on
title('Delta peso neurona salida vs alpha')
 
 %% MODIFICACION BETA
 %
 %%
disp('Modificacion de beta');
indice=1;
alpha=0.5;
for beta=0.1:0.1:1
    Wi=Wx;
    iteraciones=0;
    Yf=zeros(1,4);
    error=zeros();
    b=0;
    jj=0;
    kk=0;
    d=0;
    Wp=Wi;
    res=[];
    while d<t
        tic;
        iteraciones=iteraciones+1;
        [Yf,Wf,ex,ec,d,jj,kk,b]=calculateBP(Xi,Wi,Yi,alpha,beta,Yf,d,b,jj,kk);
        Wi=Wf;
        if kk>rep
            break
        end
        res(iteraciones,:)=[Yf ex ec d];
        tiempo_usado=toc;
    end
    Ya(indice,1)=[ec];
    Ya(indice,2)=[tiempo_usado];
    Ya(indice,3)=[iteraciones];
    Ya(indice,4)=abs(Wx(9)-Wf(9));
    indice=indice+1;
end

x=0.1:0.1:1;
figure('units','normalized','outerposition',[0 0 1 1])
subplot(2,2,1)
y1=Ya(:,1);
plot(x,y1);
grid on
title('Error cuadrático vs beta')

subplot(2,2,2)
y2=Ya(:,2);
plot(x,y2);
grid on
title('Tiempo usado vs beta')

subplot(2,2,3)
y3=Ya(:,3);
plot(x,y3);
grid on
title('Iteraciones vs beta')

subplot(2,2,4)
y4=Ya(:,4);
plot(x,y4);
grid on
title('Delta peso neurona salida vs beta')

%%  CONCLUSIONES
% # En términos generales, se debe establecer un criterio para asignar la diferencia entre las salidas, el número de pasos, la
% tasa de aprendizaje y la polarización, acorde a las necesidades para tener el resultado deseado  
% # Para el valor seleccionado de alpha (0,5) los mejores valores de beta se encuentran entre 0.3 y 0.4
% # Para el valor seleccionado de beta (1) los mejores valores de alpha se encuentran entre 0.3 y 0.4
##### SOURCE END #####
--></body></html>